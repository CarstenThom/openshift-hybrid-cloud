[OSEv3:children]
masters
nodes
etcd
nfs
#glusterfs

[masters]
oo-master-0.${MY_LOCALDOMAIN} openshift_ip=192.168.3.100 openshift_schedulable=true

# [master-0]

[etcd]
oo-master-0.${MY_LOCALDOMAIN} openshift_ip=192.168.3.100

[nodes]
oo-master-0.${MY_LOCALDOMAIN} openshift_ip=192.168.3.100 openshift_node_labels="{'region': 'infra','zone': 'default'}" openshift_schedulable=true
oo-infra-node-1.${MY_LOCALDOMAIN} openshift_ip=192.168.3.111 openshift_node_labels="{'region': 'primary','zone': 'default'}" openshift_schedulable=true
oo-app-node-01.${MY_LOCALDOMAIN} openshift_ip=192.168.3.121 openshift_node_labels="{'region': 'primary','zone': 'default'}" openshift_schedulable=true
#oo-app-node-02.${MY_LOCALDOMAIN} openshift_ip=192.168.3.122 openshift_node_labels="{'region': 'primary','zone': 'default'}" openshift_schedulable=true

#[nfsservers]
#oo-infra-node-1.${MY_LOCALDOMAIN}

[nfs]
#oo-master-0.${MY_LOCALDOMAIN} 
oo-infra-node-1.${MY_LOCALDOMAIN}

#[glusterfs]
#oo-master-0.${MY_LOCALDOMAIN} glusterfs_ip=192.168.3.100 glusterfs_devices='[ "/dev/sdb" ]'
#oo-infra-node-1.${MY_LOCALDOMAIN} glusterfs_ip=192.168.3.111 glusterfs_devices='[ "/dev/sdb" ]'
#oo-app-node-01.${MY_LOCALDOMAIN} glusterfs_ip=192.168.3.121 glusterfs_devices='[ "/dev/sdb" ]'


[OSEv3:vars]
###############################################################################
# Common/ Required configuration variables follow                             #
###############################################################################

# package 3.9.0* not found needed? no more
#openshift_repos_enable_testing=true

# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_user=root
# ?
ansible_ssh_user=root

# If ansible_user is not root, ansible_become must be set to true and the
# user must be configured for passwordless sudo
#ansible_become=yes

enable_excluders=False
enable_docker_excluder=False

containerized=True
openshift_clock_enabled=true

# Docker
container_runtime_docker_storage_setup_device=
container_runtime_docker_storage_type=overlay2
#openshift_docker_use_system_container=False
#openshift_node_local_quota_per_fsgroup=512Mi
#openshift_use_system_containers=False

### network ###
# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
# Service address space,  /16 = 65,534 IPs
openshift_portal_net=172.30.0.0/16
# Pod address space
osm_cluster_network_cidr=10.128.0.0/14
# Subnet Length of each node, 9 = 510 IPs
osm_host_subnet_length=9

### firewall ###
# iptables deprecated, for new istallations use firewalld 
#os_firewall_use_firewalld=True
# You can open additional firewall ports by defining them as a list. of service
# names and ports/port ranges for either masters or nodes.
# Prometheus [9100]
#openshift_master_open_ports=[{"service":"node-exporter","port":"9100/tcp"}]
#openshift_node_open_ports=[{"service":"node-exporter","port":"9100/tcp"},{"service":"haproxy","port":"1936/tcp"}]
openshift_node_open_ports=[{"service":"nfs","port":"2049/tcp"},{"service":"portmapper","port":"111/tcp"}]

### requirements-checks ###
openshift_disable_check=docker_image_availability
#openshift_disable_check=disk_availability,docker_storage,memory_availability,docker_image_availability

# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
# in this case: nfs
openshift_enable_unsupported_configurations=true
# NFS check bug
skip_sanity_checks=true

## Other vars
openshift_node_kubelet_args="{'image-pull-progress-deadline': ['20m'], 'eviction-hard': ['memory.available<100Mi'], 'minimum-container-ttl-duration': ['30s'], 'maximum-dead-containers-per-container': ['2'], 'maximum-dead-containers': ['5'], 'pods-per-core': ['10'], 'max-pods': ['25'], 'image-gc-high-threshold': ['80'], 'image-gc-low-threshold': ['60']}"
deployment_type=origin
openshift_deployment_type=origin
openshift_public_hostname=console.${MY_PUBLICDOMAIN}
openshift_master_default_subdomain=apps.${MY_PUBLICDOMAIN}
# Configure master API and console ports (default 8443)
openshift_master_api_port=${API_PORT}
openshift_master_console_port=${API_PORT}
#openshift_master_cluster_method=native
# Set cluster_hostname to point at your load balancer
#openshift_master_cluster_hostname=ose3-lb.test.example.com

openshift_release=v3.9
# https://github.com/openshift/openshift-ansible/pull/7370
# https://github.com/openshift/origin/tree/v3.9.0/images


### Storage Options ###

# Registry Storage Options
#
# NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume
# path using these options would be "/exports/registry".  "exports" is
# is the name of the export served by the nfs server.  "registry" is
# the name of a directory inside of "/exports".
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character
openshift_hosted_registry_storage_nfs_directory=/exports/openshift
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=50Gi

# Logging storage config
# Option A - NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume
# path using these options would be "/exports/logging".  "exports" is
# is the name of the export served by the nfs server.  "logging" is
# the name of a directory inside of "/exports".
openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/exports/openshift
openshift_logging_storage_nfs_options='*(rw,root_squash)'
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
openshift_logging_storage_labels={'storage': 'logging'}

# images
openshift_pkg_version=-${VERSION}
openshift_image_tag=v${VERSION}
#openshift_service_catalog_image_version=v${VERSION}
#? will fail when not latest?
openshift_service_catalog_image_version=latest
#template_service_broker_image_version=v${VERSION}
#openshift_metrics_image_version="v3.9"
#openshift_logging_image_version="v3.9"
#openshift_logging_elasticsearch_proxy_image_version="v1.0.0"
#?openshift_logging_elasticsearch_proxy_image_version=v1.1.0

# Enable cockpit
osm_use_cockpit=true

# Metrics deployment
# Prometheus deployment
# Currently prometheus deployment is disabled by default, enable it by setting this
openshift_hosted_prometheus_deploy=true
# Prometheus storage config
# By default prometheus uses emptydir storage, if you want to persist you should
# configure it to use pvc storage type. Each volume must be ReadWriteOnce.
openshift_prometheus_storage_type=emptydir
openshift_prometheus_alertmanager_storage_type=emptydir
openshift_prometheus_alertbuffer_storage_type=emptydir
openshift_prometheus_args=['--storage.tsdb.retention=6h', '--query.timeout=2m']
openshift_prometheus_node_selector={"region":"infra"}
openshift_prometheus_hostname=metrics.${MY_PUBLICDOMAIN}
openshift_prometheus_alerts_hostname=alerts.${MY_PUBLICDOMAIN}
# Use PVCs for persistence
#openshift_prometheus_storage_type=pvc
#openshift_prometheus_alertmanager_storage_type=pvc
#openshift_prometheus_alertbuffer_storage_type=pvc
# Grafana deployment, requires Prometheus
openshift_hosted_grafana_deploy=true
# The OpenShift-Ansible installer will fail when it detects that the
# value of openshift_hostname resolves to an IP address not bound to any local
# interfaces. This mis-configuration is problematic for any pod leveraging host
# networking and liveness or readiness probes.
# Setting this variable to false will override that check.
#openshift_hostname_check=false

# Logging deployment
#
# Currently logging deployment is disabled by default, enable it by setting this
openshift_logging_install_logging=${LOGGING}
logging_elasticsearch_rollout_override=false
# Configure the number of elastic search nodes, unless you're using dynamic provisioning this value must be 1
openshift_logging_es_cluster_size=1

# htpasswd auth
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

### template_service ###
# Enable service catalog
openshift_enable_service_catalog=true
#openshift_service_catalog_image_prefix=openshift/origin-
# Enable template service broker (requires service catalog to be enabled, above)
template_service_broker_install=true
template_service_broker_selector={"region":"infra"}
# TSB image tag
#template_service_broker_version="v3.9"
# Configure one of more namespaces whose templates will be served by the TSB
openshift_template_service_broker_namespaces=['openshift']
# Ansible Service Broker
ansible_service_broker_install=True
ansible_service_broker_registry_tag=latest
ansible_service_broker_registry_whitelist=['.*-apb$']
ansible_service_broker_local_registry_whitelist=[".*-apb$"]
#ansible_service_broker_image_prefix="ansibleplaybookbundle/origin-"
#ansible_service_broker_image_prefix=ansibleplaybookbundle/
#ansible_service_broker_image_tag=latest
ansible_service_broker_image_tag="v3.9"
ansible_service_broker_refresh_interval=20s
ansible_service_broker_registry_name=dh
ansible_service_broker_registry_url=""
ansible_service_broker_registry_type=dockerhub
ansible_service_broker_registry_organization=ansibleplaybookbundle
ansible_service_broker_etcd_image_prefix=quay.io/coreos/
ansible_service_broker_etcd_image_tag=latest
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_nfs_directory=/exports/openshift
openshift_hosted_etcd_storage_volume_name=etcd
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_host=oo-infra-node-1.${MY_LOCALDOMAIN} 
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}

######## TLS Wildcard Certificates ACMEv2 ##############

#openshift_master_overwrite_named_certificates=true
#openshift_master_named_certificates=[{"certfile": "{{ inventory_dir }}/*.apps.${MY_PUBLICDOMAIN}.cer", "keyfile": "{{ inventory_dir }}/*.apps.${MY_PUBLICDOMAIN}.key", "names": ["oo-master-0.${MY_LOCALDOMAIN}"], "cafile": "{{ inventory_dir }}/ca.cer"}]

############## sonstiges/später ########################

# Specify exact version of Docker to configure or upgrade to.
# Downgrades are not supported and will error out. Be careful when upgrading docker from < 1.10 to > 1.10.
# docker_version="1.12.1"

# default project node selector
#osm_default_node_selector='region=primary'

# StorageClass
# openshift_storageclass_name=gp2
# openshift_storageclass_parameters={'type': 'gp2', 'encrypted': 'false'}
# openshift_storageclass_mount_options=['dir_mode=0777', 'file_mode=0777']
# openshift_storageclass_reclaim_policy="Delete"



######################################################################
# CloudForms Management Engine (ManageIQ) App Install
#
# Enables installation of MIQ server. Recommended for dedicated
# clusters only. See roles/openshift_management/README.md for instructions
# and requirements.
#openshift_management_install_management=False

# CloudForms/ManageIQ (CFME/MIQ) Configuration

# See the readme for full descriptions and getting started
# instructions: ../../roles/openshift_management/README.md or go directly to
# their definitions: ../../roles/openshift_management/defaults/main.yml
# ../../roles/openshift_management/vars/main.yml
#
# Namespace for the CFME project
#openshift_management_project: openshift-management

# Namespace/project description
#openshift_management_project_description: CloudForms Management Engine

# Choose 'miq-template' for a podified database install
# Choose 'miq-template-ext-db' for an external database install
#
# If you are using the miq-template-ext-db template then you must add
# the required database parameters to the
# openshift_management_template_parameters variable.
#openshift_management_app_template: miq-template

# Allowed options: nfs, nfs_external, preconfigured, cloudprovider.
#openshift_management_storage_class: nfs

#openshift_metrics_install_metrics=${METRICS}
#openshift_metrics_hawkular_hostname=metrics.${MY_PUBLICDOMAIN}
